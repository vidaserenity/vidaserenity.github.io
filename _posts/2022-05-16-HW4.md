---
layout: post
title:  "Homework 4"
categories: blog assignment
permalink: posts/blog-post-4
author: Vida Serenity 
---

In this Blog Post, you will develop and assess a fake news classifier using Tensorflow.

## §1. Acquire Training Data

You will be working with the data set at the below URL.
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

Note that:
1. Each row of the data corresponds to an article. 
2. The title column gives the title of the article, while the text column gives the full article text. 
3. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.

Remember to start with some importing (feel free to add on)
```python
import os
import re
import string
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import utils, datasets, layers, models, Input, losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization, StringLookup

import matplotlib.pyplot as plt
import plotly.express as px 
import numpy as np
import pandas as pd

import nltk
from nltk.corpus import stopwords
from gensim.utils import simple_preprocess

nltk.download('stopwords')

from sklearn.decomposition import PCA
```

Then, read in your data and inspect it. 
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
news = pd.read_csv(train_url, index_col = 0)
news
```
![HW4_1.png]({{ site.baseurl }}/images/HW4_1.png)

## §2. Make a Dataset
Next, we want to clean up and prepare the data. Let's start by creating a function that does the following:
1. Remove stopwords from the article text and title. 
2. Construct and return a tf.data.Dataset with two inputs and one output. 

```python
# Remove Stopwords
stop = stopwords.words('english')

def remove_stopwords(texts):
  return [' '.join([word for word in simple_preprocess(str(doc)) if word not in stop]) for doc in texts]
  
# Construct and return a tf.data.Dataset with two inputs and one output
def make_dataset(data):
  data['title'] = remove_stopwords(data['title'])
  data['text'] = remove_stopwords(data['text'])
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
            "title" : data[["title"]],
            "text" : data[["text"]]
        },
        {
            "fake":data[["fake"]]
        }
      )
  )

  return data.batch(100)
  
data = make_dataset(news)
```

> Validation Data
Now, we can shuffle and split our data into the training and validation data.
```python
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
test = data.skip(train_size*val_size)
```

> Base Rate
Base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Let'setermine the base rate for this data set by examining the labels on the training set.

```python
labels_iterator = train.unbatch().map(lambda text,fake:fake).as_numpy_iterator()

fake = 0
not_fake = 0

for labels in (labels_iterator):
  if labels['fake'] == 1:
    fake += 1
  else:
      not_fake += 1

sum = fake + not_fake
(fake/sum), (not_fake/sum)
```
> Text Vectorization
Also, when working with NLP models, we are required to vectorize the text and transform words into integers so that our model can make sense of the data. We can do so using TextVectorization.

```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))

text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

## §3. Create Models
Now, for the fun part, we can start building the models to answer the questions below.
> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?
For our purposes, we will be building 3 different models.

### Model 1: Use only the article title as input
```python
title_features = title_vectorize_layer(titles_input)
title_features = layers.Embedding(size_vocabulary, output_dim = 10, name = "embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

output = layers.Dense(2, name = "fake")(title_features)

model1 = tf.keras.Model(
    inputs = titles_input,
    outputs = output
)

keras.utils.plot_model(model1)
```
![HW4_2.png]({{ site.baseurl }}/images/HW4_2.png)

```python
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])

history = model1.fit(train, 
                    validation_data=val,
                    epochs = 50)
```
Output:
```python
Epoch 1/50
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
180/180 [==============================] - 2s 6ms/step - loss: 0.6920 - accuracy: 0.5224 - val_loss: 0.6909 - val_accuracy: 0.5244
Epoch 2/50
180/180 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.5414 - val_loss: 0.6803 - val_accuracy: 0.5096
Epoch 3/50
180/180 [==============================] - 1s 6ms/step - loss: 0.6307 - accuracy: 0.7271 - val_loss: 0.5519 - val_accuracy: 0.8320
Epoch 4/50
180/180 [==============================] - 1s 6ms/step - loss: 0.4596 - accuracy: 0.8448 - val_loss: 0.3637 - val_accuracy: 0.8840
Epoch 5/50
180/180 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8831 - val_loss: 0.2679 - val_accuracy: 0.9060
Epoch 6/50
180/180 [==============================] - 1s 6ms/step - loss: 0.2614 - accuracy: 0.9042 - val_loss: 0.2285 - val_accuracy: 0.9180
Epoch 7/50
180/180 [==============================] - 1s 5ms/step - loss: 0.2195 - accuracy: 0.9171 - val_loss: 0.2007 - val_accuracy: 0.9207
Epoch 8/50
180/180 [==============================] - 1s 5ms/step - loss: 0.2009 - accuracy: 0.9226 - val_loss: 0.1914 - val_accuracy: 0.9191
Epoch 9/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1847 - accuracy: 0.9275 - val_loss: 0.1876 - val_accuracy: 0.9242
Epoch 10/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1723 - accuracy: 0.9326 - val_loss: 0.1545 - val_accuracy: 0.9416
Epoch 11/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1658 - accuracy: 0.9340 - val_loss: 0.1569 - val_accuracy: 0.9384
Epoch 12/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1605 - accuracy: 0.9363 - val_loss: 0.1369 - val_accuracy: 0.9462
Epoch 13/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1555 - accuracy: 0.9376 - val_loss: 0.1469 - val_accuracy: 0.9442
Epoch 14/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.1305 - val_accuracy: 0.9469
Epoch 15/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1442 - accuracy: 0.9428 - val_loss: 0.1354 - val_accuracy: 0.9453
Epoch 16/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1447 - accuracy: 0.9432 - val_loss: 0.1307 - val_accuracy: 0.9511
Epoch 17/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1411 - accuracy: 0.9440 - val_loss: 0.1277 - val_accuracy: 0.9484
Epoch 18/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1336 - accuracy: 0.9471 - val_loss: 0.1206 - val_accuracy: 0.9530
Epoch 19/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1323 - accuracy: 0.9461 - val_loss: 0.1254 - val_accuracy: 0.9498
Epoch 20/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1317 - accuracy: 0.9476 - val_loss: 0.1137 - val_accuracy: 0.9558
Epoch 21/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1293 - accuracy: 0.9505 - val_loss: 0.1069 - val_accuracy: 0.9571
Epoch 22/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1298 - accuracy: 0.9494 - val_loss: 0.1171 - val_accuracy: 0.9549
Epoch 23/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1237 - accuracy: 0.9515 - val_loss: 0.1100 - val_accuracy: 0.9568
Epoch 24/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1222 - accuracy: 0.9526 - val_loss: 0.1093 - val_accuracy: 0.9567
Epoch 25/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1230 - accuracy: 0.9520 - val_loss: 0.1359 - val_accuracy: 0.9436
Epoch 26/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1245 - accuracy: 0.9510 - val_loss: 0.1027 - val_accuracy: 0.9618
Epoch 27/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1224 - accuracy: 0.9524 - val_loss: 0.0960 - val_accuracy: 0.9647
Epoch 28/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1200 - accuracy: 0.9518 - val_loss: 0.1140 - val_accuracy: 0.9571
Epoch 29/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1167 - accuracy: 0.9537 - val_loss: 0.1053 - val_accuracy: 0.9580
Epoch 30/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1153 - accuracy: 0.9542 - val_loss: 0.1069 - val_accuracy: 0.9587
Epoch 31/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1129 - accuracy: 0.9556 - val_loss: 0.1049 - val_accuracy: 0.9593
Epoch 32/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1181 - accuracy: 0.9543 - val_loss: 0.0993 - val_accuracy: 0.9633
Epoch 33/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1158 - accuracy: 0.9547 - val_loss: 0.0985 - val_accuracy: 0.9634
Epoch 34/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1138 - accuracy: 0.9542 - val_loss: 0.1125 - val_accuracy: 0.9549
Epoch 35/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1136 - accuracy: 0.9545 - val_loss: 0.0965 - val_accuracy: 0.9636
Epoch 36/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1122 - accuracy: 0.9559 - val_loss: 0.0959 - val_accuracy: 0.9653
Epoch 37/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1121 - accuracy: 0.9565 - val_loss: 0.1013 - val_accuracy: 0.9593
Epoch 38/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1108 - accuracy: 0.9565 - val_loss: 0.1038 - val_accuracy: 0.9611
Epoch 39/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1088 - accuracy: 0.9580 - val_loss: 0.1017 - val_accuracy: 0.9609
Epoch 40/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1081 - accuracy: 0.9583 - val_loss: 0.0989 - val_accuracy: 0.9644
Epoch 41/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1085 - accuracy: 0.9587 - val_loss: 0.0902 - val_accuracy: 0.9667
Epoch 42/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1075 - accuracy: 0.9573 - val_loss: 0.0895 - val_accuracy: 0.9651
Epoch 43/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1125 - accuracy: 0.9568 - val_loss: 0.1082 - val_accuracy: 0.9600
Epoch 44/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1072 - accuracy: 0.9592 - val_loss: 0.0946 - val_accuracy: 0.9631
Epoch 45/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1096 - accuracy: 0.9564 - val_loss: 0.1092 - val_accuracy: 0.9567
Epoch 46/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1070 - accuracy: 0.9598 - val_loss: 0.0935 - val_accuracy: 0.9618
Epoch 47/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1082 - accuracy: 0.9579 - val_loss: 0.0991 - val_accuracy: 0.9620
Epoch 48/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1044 - accuracy: 0.9603 - val_loss: 0.0913 - val_accuracy: 0.9661
Epoch 49/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1051 - accuracy: 0.9588 - val_loss: 0.1073 - val_accuracy: 0.9587
Epoch 50/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1086 - accuracy: 0.9587 - val_loss: 0.1034 - val_accuracy: 0.9598
```
As you can see, the model achieves around a 95% accuracy on the validation data. 
Let's plot this on a graph for better visualization.
```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![HW4_3.png]({{ site.baseurl }}/images/HW4_3.png)

### #Model 2: Use only article text as input 
```python 
text_features = text_vectorize_layer(texts_input)
text_features = layers.Embedding(size_vocabulary, output_dim = 10, name = "embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

output = layers.Dense(2, name = "fake")(text_features)

model2 = tf.keras.Model(
    inputs = texts_input,
    outputs = output
)

keras.utils.plot_model(model2)
```
![HW4_4.png]({{ site.baseurl }}/images/HW4_4.png)

```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])

history = model2.fit(train, 
                    validation_data=val,
                    epochs = 50)
```
Output:
```python
Epoch 1/50
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
180/180 [==============================] - 3s 11ms/step - loss: 0.6369 - accuracy: 0.7035 - val_loss: 0.4793 - val_accuracy: 0.8933
Epoch 2/50
180/180 [==============================] - 2s 10ms/step - loss: 0.3236 - accuracy: 0.9222 - val_loss: 0.2248 - val_accuracy: 0.9462
Epoch 3/50
180/180 [==============================] - 2s 10ms/step - loss: 0.1877 - accuracy: 0.9510 - val_loss: 0.1419 - val_accuracy: 0.9696
Epoch 4/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1461 - accuracy: 0.9614 - val_loss: 0.1222 - val_accuracy: 0.9702
Epoch 5/50
180/180 [==============================] - 3s 17ms/step - loss: 0.1233 - accuracy: 0.9665 - val_loss: 0.1122 - val_accuracy: 0.9673
Epoch 6/50
180/180 [==============================] - 3s 17ms/step - loss: 0.1055 - accuracy: 0.9722 - val_loss: 0.1024 - val_accuracy: 0.9733
Epoch 7/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0936 - accuracy: 0.9754 - val_loss: 0.0879 - val_accuracy: 0.9822
Epoch 8/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0858 - accuracy: 0.9778 - val_loss: 0.0841 - val_accuracy: 0.9807
Epoch 9/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0765 - accuracy: 0.9802 - val_loss: 0.0668 - val_accuracy: 0.9822
Epoch 10/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0709 - accuracy: 0.9816 - val_loss: 0.0584 - val_accuracy: 0.9864
Epoch 11/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0685 - accuracy: 0.9822 - val_loss: 0.0642 - val_accuracy: 0.9851
Epoch 12/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0633 - accuracy: 0.9838 - val_loss: 0.0713 - val_accuracy: 0.9816
Epoch 13/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0596 - accuracy: 0.9850 - val_loss: 0.0435 - val_accuracy: 0.9913
Epoch 14/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0550 - accuracy: 0.9854 - val_loss: 0.0432 - val_accuracy: 0.9896
Epoch 15/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0488 - accuracy: 0.9875 - val_loss: 0.0452 - val_accuracy: 0.9896
Epoch 16/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0485 - accuracy: 0.9881 - val_loss: 0.0374 - val_accuracy: 0.9897
Epoch 17/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0455 - accuracy: 0.9894 - val_loss: 0.0374 - val_accuracy: 0.9910
Epoch 18/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0422 - accuracy: 0.9904 - val_loss: 0.0439 - val_accuracy: 0.9904
Epoch 19/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0406 - accuracy: 0.9903 - val_loss: 0.0381 - val_accuracy: 0.9893
Epoch 20/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.0225 - val_accuracy: 0.9956
Epoch 21/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0364 - accuracy: 0.9920 - val_loss: 0.0314 - val_accuracy: 0.9942
Epoch 22/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0337 - accuracy: 0.9920 - val_loss: 0.0255 - val_accuracy: 0.9936
Epoch 23/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0311 - accuracy: 0.9918 - val_loss: 0.0247 - val_accuracy: 0.9957
Epoch 24/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0324 - accuracy: 0.9926 - val_loss: 0.0209 - val_accuracy: 0.9951
Epoch 25/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0307 - accuracy: 0.9919 - val_loss: 0.0231 - val_accuracy: 0.9951
Epoch 26/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0302 - accuracy: 0.9925 - val_loss: 0.0256 - val_accuracy: 0.9940
Epoch 27/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0278 - accuracy: 0.9930 - val_loss: 0.0198 - val_accuracy: 0.9946
Epoch 28/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0273 - accuracy: 0.9931 - val_loss: 0.0271 - val_accuracy: 0.9962
Epoch 29/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0274 - accuracy: 0.9931 - val_loss: 0.0216 - val_accuracy: 0.9973
Epoch 30/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0224 - accuracy: 0.9946 - val_loss: 0.0212 - val_accuracy: 0.9956
Epoch 31/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.0172 - val_accuracy: 0.9964
Epoch 32/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0232 - accuracy: 0.9941 - val_loss: 0.0137 - val_accuracy: 0.9964
Epoch 33/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0225 - accuracy: 0.9945 - val_loss: 0.0181 - val_accuracy: 0.9969
Epoch 34/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0214 - accuracy: 0.9947 - val_loss: 0.0119 - val_accuracy: 0.9976
Epoch 35/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0202 - accuracy: 0.9950 - val_loss: 0.0147 - val_accuracy: 0.9982
Epoch 36/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0214 - accuracy: 0.9940 - val_loss: 0.0142 - val_accuracy: 0.9969
Epoch 37/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 0.0169 - val_accuracy: 0.9962
Epoch 38/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.0125 - val_accuracy: 0.9980
Epoch 39/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0192 - accuracy: 0.9952 - val_loss: 0.0170 - val_accuracy: 0.9973
Epoch 40/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0162 - accuracy: 0.9960 - val_loss: 0.0105 - val_accuracy: 0.9976
Epoch 41/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0147 - val_accuracy: 0.9969
Epoch 42/50
180/180 [==============================] - 2s 9ms/step - loss: 0.0175 - accuracy: 0.9953 - val_loss: 0.0136 - val_accuracy: 0.9984
Epoch 43/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.0112 - val_accuracy: 0.9953
Epoch 44/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0109 - val_accuracy: 0.9971
Epoch 45/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.0084 - val_accuracy: 0.9987
Epoch 46/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.0067 - val_accuracy: 0.9989
Epoch 47/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0151 - accuracy: 0.9963 - val_loss: 0.0085 - val_accuracy: 0.9984
Epoch 48/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0157 - accuracy: 0.9958 - val_loss: 0.0074 - val_accuracy: 0.9987
Epoch 49/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0156 - accuracy: 0.9958 - val_loss: 0.0122 - val_accuracy: 0.9976
Epoch 50/50
180/180 [==============================] - 2s 10ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0090 - val_accuracy: 0.9989
```
As you can see, the model achieves around a 99% accuracy on the validation data. 
Let's plot this on a graph for better visualization using the same code from model 1.
![HW4_5.png]({{ site.baseurl }}/images/HW4_5.png)

### #Model 3: Use both the article title and the article text as input
```python
# combine text and title
main = layers.concatenate([title_features, text_features], axis = 1)

# output
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)

model3 = keras.Model(
    inputs = [titles_input, texts_input],
    outputs = output
)

keras.utils.plot_model(model3)
```
![HW4_6.png]({{ site.baseurl }}/images/HW4_6.png)

```python
model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = model3.fit(train, 
                    validation_data=val,
                    epochs = 50)
```
Output:
```python
Epoch 1/50
180/180 [==============================] - 3s 13ms/step - loss: 0.1763 - accuracy: 0.9465 - val_loss: 0.0348 - val_accuracy: 0.9969
Epoch 2/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0281 - accuracy: 0.9944 - val_loss: 0.0140 - val_accuracy: 0.9993
Epoch 3/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0174 - accuracy: 0.9966 - val_loss: 0.0091 - val_accuracy: 0.9987
Epoch 4/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0113 - accuracy: 0.9978 - val_loss: 0.0049 - val_accuracy: 0.9998
Epoch 5/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0050 - val_accuracy: 0.9987
Epoch 6/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0040 - val_accuracy: 0.9987
Epoch 7/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0023 - val_accuracy: 0.9996
Epoch 8/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0025 - val_accuracy: 0.9993
Epoch 9/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0012 - val_accuracy: 0.9998
Epoch 10/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0025 - val_accuracy: 0.9996
Epoch 11/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0022 - val_accuracy: 0.9998
Epoch 12/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0013 - val_accuracy: 0.9996
Epoch 13/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0025 - val_accuracy: 0.9993
Epoch 14/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 0.9998
Epoch 15/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 5.4667e-04 - val_accuracy: 0.9998
Epoch 16/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0014 - val_accuracy: 0.9996
Epoch 17/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0028 - val_accuracy: 0.9996
Epoch 18/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 5.2760e-04 - val_accuracy: 1.0000
Epoch 19/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0018 - val_accuracy: 0.9998
Epoch 20/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0019 - val_accuracy: 0.9998
Epoch 21/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0016 - val_accuracy: 0.9998
Epoch 22/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 1.7998e-04 - val_accuracy: 1.0000
Epoch 23/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 3.4755e-04 - val_accuracy: 1.0000
Epoch 24/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0012 - val_accuracy: 0.9998
Epoch 25/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 2.7374e-04 - val_accuracy: 1.0000
Epoch 26/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 1.7138e-04 - val_accuracy: 1.0000
Epoch 27/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.0012 - val_accuracy: 0.9998
Epoch 28/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 7.2784e-05 - val_accuracy: 1.0000
Epoch 29/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 8.5682e-04 - val_accuracy: 0.9998
Epoch 30/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.5755e-04 - val_accuracy: 1.0000
Epoch 31/50
180/180 [==============================] - 2s 11ms/step - loss: 7.9835e-04 - accuracy: 0.9997 - val_loss: 6.7393e-05 - val_accuracy: 1.0000
Epoch 32/50
180/180 [==============================] - 2s 12ms/step - loss: 5.5832e-04 - accuracy: 0.9999 - val_loss: 4.4261e-04 - val_accuracy: 0.9998
Epoch 33/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.7476e-04 - val_accuracy: 1.0000
Epoch 34/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 5.3928e-04 - val_accuracy: 0.9998
Epoch 35/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0045 - val_accuracy: 0.9984
Epoch 36/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 5.3255e-05 - val_accuracy: 1.0000
Epoch 37/50
180/180 [==============================] - 2s 12ms/step - loss: 8.6501e-04 - accuracy: 0.9997 - val_loss: 6.7222e-05 - val_accuracy: 1.0000
Epoch 38/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 4.5690e-05 - val_accuracy: 1.0000
Epoch 39/50
180/180 [==============================] - 2s 11ms/step - loss: 5.7530e-04 - accuracy: 0.9998 - val_loss: 2.5234e-05 - val_accuracy: 1.0000
Epoch 40/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 5.7495e-04 - val_accuracy: 1.0000
Epoch 41/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 4.9235e-04 - val_accuracy: 0.9998
Epoch 42/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0010 - val_accuracy: 0.9998
Epoch 43/50
180/180 [==============================] - 2s 12ms/step - loss: 4.8930e-04 - accuracy: 0.9999 - val_loss: 1.3522e-04 - val_accuracy: 1.0000
Epoch 44/50
180/180 [==============================] - 2s 12ms/step - loss: 5.2932e-04 - accuracy: 0.9998 - val_loss: 4.3500e-04 - val_accuracy: 0.9998
Epoch 45/50
180/180 [==============================] - 2s 11ms/step - loss: 9.9732e-04 - accuracy: 0.9996 - val_loss: 3.9921e-05 - val_accuracy: 1.0000
Epoch 46/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 8.1110e-04 - val_accuracy: 0.9998
Epoch 47/50
180/180 [==============================] - 2s 12ms/step - loss: 7.1509e-04 - accuracy: 0.9998 - val_loss: 3.8066e-05 - val_accuracy: 1.0000
Epoch 48/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 5.1782e-05 - val_accuracy: 1.0000
Epoch 49/50
180/180 [==============================] - 2s 12ms/step - loss: 5.9107e-04 - accuracy: 0.9998 - val_loss: 5.5469e-05 - val_accuracy: 1.0000
Epoch 50/50
180/180 [==============================] - 2s 11ms/step - loss: 1.0197e-04 - accuracy: 1.0000 - val_loss: 1.0563e-05 - val_accuracy: 1.0000
```
As you can see, the model achieves a 100% accuracy on the validation data. 
Again, let's plot this on a graph for better visualization using the same code from model 1.
![HW4_7.png]({{ site.baseurl }}/images/HW4_7.png)

## §4. Model Evaluation
Now that we have finished building our models, we can try testing it against unseen data. 
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_news = pd.read_csv(test_url, index_col = 0)
test_data = make_dataset(test_news)

model3.evaluate(test_data)
```
Output:
```python
225/225 [==============================] - 2s 7ms/step - loss: 0.1076 - accuracy: 0.9844
[0.10755540430545807, 0.9843645691871643]
```
Hurray, we got an accuracy of 98.4% on the test data set. 

## §5. Embedding Visualization
Lastly, word embedding is to learn a function that maps a word to a vector. 
Let's use word embedding to understand how we go from words to numbers.

Note that here we are using a 2-dimensional embedding.
```python
weights = model1.get_layer("embedding1").get_weights()[0]
vocab = title_vectorize_layer.get_vocabulary()

pca = PCA(n_components = 2)
weights = pca.fit_transform(weights)

embedding_data = pd.DataFrame({
    'word':vocab,
    'x0':weights[:,0],
    'x1':weights[:,1],
})

embedding_data
```
![HW4_8.png]({{ site.baseurl }}/images/HW4_8.png)

Now, plot the data as a scatterplot. 
```python
fig = px.scatter(embedding_data, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_data),
                 hover_name = "word")

fig.show()
```
![HW4_9.png]({{ site.baseurl }}/images/HW4_9.png)

As you can see, majority of the words are clustered up in the middle.
1. The rightmost point is “video” while the leftmost point is “factbox”. This is because the way articles are vectorized in our model would be if the title either includes factbox or video.
2. “rohingya” and “myanmar” are located pretty close to each other. This is because of the media coverage on Rohingya refugee crisis which is in Myanmar.
3. "feds" and "cops" are also located close to each other. This is because their definition are similar in a sense of context.

