---
layout: post
title:  "Homework 3"
categories: blog assignment
permalink: posts/blog-post-3
author: Vida Serenity 
---

> Teaching a machine learning algorithm to distinguish between pictures of dogs and pictures of cats

In this blog post, you will learn several new skills and concepts related to image classification in Tensorflow.
- Tensorflow Datasets provide a convenient way for us to organize operations on our training, validation, and test data sets.
- Data augmentation allows us to create expanded versions of our data sets that allow models to learn patterns more robustly.
- Transfer learning allows us to use pre-trained models for new tasks.

## §1. Load Packages and Obtain Data

Start by importing the required libraries
```python
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import utils, datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
```

Create TensorFlow Datasets for training, validation, and testing. You can think of a Dataset as a pipeline that feeds data to a machine learning model. We use data sets in cases in which it’s not necessarily practical to load all the data into memory.
```python

# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

Next, run the following code for rapidly reading data. If you’re interested in learning more about this kind of thing, you can take a look [here](https://www.tensorflow.org/guide/data_performance).
```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Let's briefly explore our data set. 
Note that training data with label 0 (corresponding to "cat") and label 1 (corresponding to "dog").
```python
def printimages(dataset):
  plt.figure(figsize=(10, 10))

  for images, labels in dataset.take(1):
    for i in range(3):
      ax = plt.subplot(2,3, i+1)
      index = np.where(labels == 0)[0][i]
      plt.imshow(images[index].numpy().astype("uint8"))
      plt.title("cat")
      plt.axis("off")

    for i in range(3):
      ax = plt.subplot(2,3, i+4)
      index = np.where(labels == 1)[0][i]
      plt.imshow(images[index].numpy().astype("uint8"))
      plt.title("dog")
      plt.axis("off")
    
    plt.show()

printimages(train_dataset)
```

Next, we should check label frequencies. 
```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_n
count_cat = 0
count_dog = 0

for label in labels_iterator:
    if label == 0:
      count_cat += 1
    elif label == 1:
      count_dog += 1

count_cat, count_dog
```
You should get the output below:
```python
(1000, 1000)
```

## §2. First Model

Now, let's start building our first model. You can include different layers in your model should as Conv2D layers, MaxPooling2D layers,Flatten layer, Dense layer, and a Dropout layer. Feel free to play around with them to see which model optimizes the accuracy. 
Then, train your model and plot the history of the accuracy on both the training and validation sets.
```python
model1 = models.Sequential([
      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3, 3), activation='relu'),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(64, (3, 3), activation='relu'), # n "pixels" x n "pixels" x 64
      layers.Flatten(), # n^2 * 64 length vector
      
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dropout(rate=0.5),
      layers.Dense(2) # number of classes in your dataset
])
```

After building your model, remember to compile it before training it with our new dataset.
```python
model1.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Output:
```python
Epoch 1/20
63/63 [==============================] - 7s 85ms/step - loss: 5.0721 - accuracy: 0.4905 - val_loss: 0.6926 - val_accuracy: 0.5248
Epoch 2/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6886 - accuracy: 0.5520 - val_loss: 0.6862 - val_accuracy: 0.5111
Epoch 3/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6629 - accuracy: 0.6195 - val_loss: 0.6934 - val_accuracy: 0.5322
Epoch 4/20
63/63 [==============================] - 5s 77ms/step - loss: 0.5806 - accuracy: 0.7005 - val_loss: 0.7243 - val_accuracy: 0.5829
Epoch 5/20
63/63 [==============================] - 5s 77ms/step - loss: 0.4490 - accuracy: 0.7995 - val_loss: 0.8372 - val_accuracy: 0.5594
Epoch 6/20
63/63 [==============================] - 5s 78ms/step - loss: 0.3242 - accuracy: 0.8725 - val_loss: 1.2424 - val_accuracy: 0.5656
Epoch 7/20
63/63 [==============================] - 6s 85ms/step - loss: 0.5696 - accuracy: 0.7570 - val_loss: 1.1742 - val_accuracy: 0.5334
Epoch 8/20
63/63 [==============================] - 5s 77ms/step - loss: 0.3909 - accuracy: 0.8250 - val_loss: 1.2930 - val_accuracy: 0.5507
Epoch 9/20
63/63 [==============================] - 5s 78ms/step - loss: 0.2054 - accuracy: 0.9165 - val_loss: 1.4435 - val_accuracy: 0.5235
Epoch 10/20
63/63 [==============================] - 5s 77ms/step - loss: 0.1483 - accuracy: 0.9480 - val_loss: 1.6969 - val_accuracy: 0.5619
Epoch 11/20
63/63 [==============================] - 5s 77ms/step - loss: 0.0978 - accuracy: 0.9635 - val_loss: 2.2486 - val_accuracy: 0.5421
Epoch 12/20
63/63 [==============================] - 5s 78ms/step - loss: 0.1235 - accuracy: 0.9630 - val_loss: 2.9268 - val_accuracy: 0.5520
Epoch 13/20
63/63 [==============================] - 5s 79ms/step - loss: 0.0551 - accuracy: 0.9800 - val_loss: 3.8215 - val_accuracy: 0.5606
Epoch 14/20
63/63 [==============================] - 5s 78ms/step - loss: 0.0436 - accuracy: 0.9880 - val_loss: 3.7677 - val_accuracy: 0.5334
Epoch 15/20
63/63 [==============================] - 5s 77ms/step - loss: 0.0322 - accuracy: 0.9905 - val_loss: 3.8511 - val_accuracy: 0.5545
Epoch 16/20
63/63 [==============================] - 5s 75ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 4.5279 - val_accuracy: 0.5557
Epoch 17/20
63/63 [==============================] - 5s 75ms/step - loss: 0.1455 - accuracy: 0.9715 - val_loss: 2.3300 - val_accuracy: 0.5446
Epoch 18/20
63/63 [==============================] - 5s 75ms/step - loss: 0.1842 - accuracy: 0.9480 - val_loss: 2.2592 - val_accuracy: 0.5545
Epoch 19/20
63/63 [==============================] - 5s 76ms/step - loss: 0.0924 - accuracy: 0.9780 - val_loss: 2.4424 - val_accuracy: 0.5656
Epoch 20/20
63/63 [==============================] - 5s 78ms/step - loss: 0.0300 - accuracy: 0.9925 - val_loss: 2.7555 - val_accuracy: 0.5866
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.show()
```
![HW3_model1.png]({{ site.baseurl }}/images/HW3_model1.png)

The accuracy of my model is stabilized **between 52% and 60%** during training.

Compared to the baseline of 52%, my model did alright, especially towards the end.

Note that Model 1 is cannot be overfitting the data as the training accurcacy is around the same as the validation accuracy.

## §3. Model with Data Augmentation

Let's try experimenting with data augmentation. Data augmentation refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called invariant features of our input images.

1. First, create a tf.keras.layers.RandomFlip() layer. 
```python
random_flip = tf.keras.Sequential([tf.keras.layers.RandomFlip()])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = random_flip(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```
![HW3_model2dog.png]({{ site.baseurl }}/images/HW3_model2dog.png)

2. Next, create a tf.keras.layers.RandomRotation() layer.
```python
random_flip = tf.keras.Sequential([tf.keras.layers.RandomRotation(0.2)])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = random_flip(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```
![HW3_model2dogg.png]({{ site.baseurl }}/images/HW3_model2dogg.png)

Let's incorporate the above layers into our model.
```python
model2 = models.Sequential([
          layers.RandomFlip("horizontal_and_vertical"),
          layers.RandomRotation(0.2),

          layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
          layers.MaxPooling2D((2, 2)), 
          layers.Conv2D(32, (3, 3), strides = (2,2), activation='relu'),
          layers.MaxPooling2D((2, 2)),
          layers.Conv2D(64, (3, 3), activation='relu'),
          layers.MaxPooling2D((2, 2)),
          layers.Conv2D(64, (3, 3), activation='relu'),

          layers.Flatten(), #input to dense must be flattened from 3d -> 1d array
          layers.Dropout(0.2), #hide some neurons 
          layers.Dense(64, activation='relu'),
          layers.Dense(2, activation='sigmoid')
])
```
```python
model2.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Output:
```python
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
  return dispatch_target(*args, **kwargs)
63/63 [==============================] - 60s 79ms/step - loss: 2.0009 - accuracy: 0.5155 - val_loss: 0.7206 - val_accuracy: 0.4926
Epoch 2/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6927 - accuracy: 0.5315 - val_loss: 0.6840 - val_accuracy: 0.5718
Epoch 3/20
63/63 [==============================] - 6s 97ms/step - loss: 0.6724 - accuracy: 0.5965 - val_loss: 0.6650 - val_accuracy: 0.6077
Epoch 4/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6651 - accuracy: 0.6080 - val_loss: 0.6643 - val_accuracy: 0.6114
Epoch 5/20
63/63 [==============================] - 5s 78ms/step - loss: 0.6683 - accuracy: 0.5975 - val_loss: 0.6582 - val_accuracy: 0.6200
Epoch 6/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6626 - accuracy: 0.5945 - val_loss: 0.6474 - val_accuracy: 0.6163
Epoch 7/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6481 - accuracy: 0.6210 - val_loss: 0.6470 - val_accuracy: 0.6386
Epoch 8/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6482 - accuracy: 0.6270 - val_loss: 0.6917 - val_accuracy: 0.5903
Epoch 9/20
63/63 [==============================] - 5s 77ms/step - loss: 0.6511 - accuracy: 0.6145 - val_loss: 0.6407 - val_accuracy: 0.6423
Epoch 10/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6357 - accuracy: 0.6465 - val_loss: 0.6495 - val_accuracy: 0.6225
Epoch 11/20
63/63 [==============================] - 5s 75ms/step - loss: 0.6235 - accuracy: 0.6430 - val_loss: 0.6081 - val_accuracy: 0.6708
Epoch 12/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6027 - accuracy: 0.6635 - val_loss: 0.6081 - val_accuracy: 0.6658
Epoch 13/20
63/63 [==============================] - 5s 75ms/step - loss: 0.6247 - accuracy: 0.6570 - val_loss: 0.5991 - val_accuracy: 0.6968
Epoch 14/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6270 - accuracy: 0.6500 - val_loss: 0.6092 - val_accuracy: 0.6832
Epoch 15/20
63/63 [==============================] - 5s 76ms/step - loss: 0.6061 - accuracy: 0.6650 - val_loss: 0.5992 - val_accuracy: 0.6943
Epoch 16/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5979 - accuracy: 0.6730 - val_loss: 0.6212 - val_accuracy: 0.6708
Epoch 17/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5906 - accuracy: 0.6855 - val_loss: 0.6166 - val_accuracy: 0.6795
Epoch 18/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5994 - accuracy: 0.6730 - val_loss: 0.5801 - val_accuracy: 0.7005
Epoch 19/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5834 - accuracy: 0.6850 - val_loss: 0.6149 - val_accuracy: 0.6597
Epoch 20/20
63/63 [==============================] - 5s 76ms/step - loss: 0.5990 - accuracy: 0.6730 - val_loss: 0.6000 - val_accuracy: 0.6881
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.show()
```
![HW3_model2.png]({{ site.baseurl }}/images/HW3_model2.png)

The accuracy of my model is stabilized **between 60% and 70%** after a few epochs during training.

Compared to the baseline of 55% and compared to the accuracy from model 1, model2 did a prety good job.

Again, note that Model 2 is cannot be overfitting the data as the training accurcacy is around the same as the validation accuracy.

## §4. Data Preprocessing

At times, it can also be helpful to make simple transformations to the input data. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. 

Let's create a preprocessing layer called preprocessor which you can slot into your model pipeline.
```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])

model3 = models.Sequential([
          preprocessor,
          layers.RandomFlip(),
          layers.RandomRotation(0.1, fill_mode='reflect'),
          layers.Conv2D(32,(3,3),activation='relu',input_shape = (160,160,3)),
          layers.MaxPooling2D((2, 2)),
          layers.Conv2D(32, (3, 3), activation='relu'),
          layers.MaxPooling2D((2, 2)),
          layers.Conv2D(64, (3, 3), activation='relu'),
          layers.Flatten(),
          layers.Dropout(rate=0.2),

          layers.Dense(64, activation='relu'),
          layers.Dense(64, activation='relu'),
          layers.Dense(2)  
])
```

```python
model3.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Output:
```python
Epoch 1/20
63/63 [==============================] - 8s 111ms/step - loss: 0.7574 - accuracy: 0.5230 - val_loss: 0.6419 - val_accuracy: 0.6126
Epoch 2/20
63/63 [==============================] - 9s 133ms/step - loss: 0.6576 - accuracy: 0.5840 - val_loss: 0.6378 - val_accuracy: 0.6275
Epoch 3/20
63/63 [==============================] - 6s 97ms/step - loss: 0.6377 - accuracy: 0.6295 - val_loss: 0.6171 - val_accuracy: 0.6436
Epoch 4/20
63/63 [==============================] - 6s 84ms/step - loss: 0.6056 - accuracy: 0.6705 - val_loss: 0.5955 - val_accuracy: 0.6795
Epoch 5/20
63/63 [==============================] - 6s 88ms/step - loss: 0.6060 - accuracy: 0.6820 - val_loss: 0.6234 - val_accuracy: 0.6609
Epoch 6/20
63/63 [==============================] - 6s 92ms/step - loss: 0.5988 - accuracy: 0.6835 - val_loss: 0.6041 - val_accuracy: 0.6634
Epoch 7/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5699 - accuracy: 0.7060 - val_loss: 0.5998 - val_accuracy: 0.6621
Epoch 8/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5543 - accuracy: 0.7095 - val_loss: 0.5832 - val_accuracy: 0.7203
Epoch 9/20
63/63 [==============================] - 6s 90ms/step - loss: 0.5469 - accuracy: 0.7155 - val_loss: 0.5772 - val_accuracy: 0.6955
Epoch 10/20
63/63 [==============================] - 6s 90ms/step - loss: 0.5382 - accuracy: 0.7180 - val_loss: 0.5429 - val_accuracy: 0.7042
Epoch 11/20
63/63 [==============================] - 6s 91ms/step - loss: 0.5342 - accuracy: 0.7270 - val_loss: 0.6042 - val_accuracy: 0.6931
Epoch 12/20
63/63 [==============================] - 6s 93ms/step - loss: 0.5192 - accuracy: 0.7400 - val_loss: 0.5504 - val_accuracy: 0.7116
Epoch 13/20
63/63 [==============================] - 6s 93ms/step - loss: 0.5142 - accuracy: 0.7465 - val_loss: 0.5388 - val_accuracy: 0.7054
Epoch 14/20
63/63 [==============================] - 6s 88ms/step - loss: 0.5093 - accuracy: 0.7510 - val_loss: 0.5452 - val_accuracy: 0.7141
Epoch 15/20
63/63 [==============================] - 6s 86ms/step - loss: 0.5044 - accuracy: 0.7530 - val_loss: 0.5821 - val_accuracy: 0.7017
Epoch 16/20
63/63 [==============================] - 6s 90ms/step - loss: 0.4815 - accuracy: 0.7720 - val_loss: 0.6348 - val_accuracy: 0.6881
Epoch 17/20
63/63 [==============================] - 6s 87ms/step - loss: 0.4895 - accuracy: 0.7620 - val_loss: 0.5826 - val_accuracy: 0.7104
Epoch 18/20
63/63 [==============================] - 6s 86ms/step - loss: 0.4597 - accuracy: 0.7780 - val_loss: 0.5696 - val_accuracy: 0.7302
Epoch 19/20
63/63 [==============================] - 6s 88ms/step - loss: 0.4746 - accuracy: 0.7685 - val_loss: 0.5970 - val_accuracy: 0.7252
Epoch 20/20
63/63 [==============================] - 6s 86ms/step - loss: 0.4696 - accuracy: 0.7650 - val_loss: 0.5822 - val_accuracy: 0.7240
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.show()
```
![HW3_model3.png]({{ site.baseurl }}/images/HW3_model3.png)

The accuracy of my model is stabilized **between 70% and 78%** starting from the seventh epochs during training.

Compared to our previous models, this model has an even higher accuracy.

Same as previous models, note that Model 3 is cannot be overfitting the data as the training accurcacy is around the same as the validation accuracy.

## §5. Transfer Learning

Next, let's try to use a pre-existing model for our task.
To do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.
Paste the following code in order to download MobileNetV2 and configure it as a layer that can be included in your model.
```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```
For this, you should definitely use the following layers:

The preprocessor layer from Part §4.
The data augmentation layers from Part §3.
The base_model_layer constructed above.
A Dense(2) layer at the very end to actually perform the classification.

```python
model4 = models.Sequential([
          preprocessor,
          layers.RandomFlip("horizontal_and_vertical"),
          layers.RandomRotation(0.2),
          base_model_layer, 
          keras.layers.GlobalMaxPooling2D(),          
          layers.Dropout(rate=0.2),
          layers.Dense(2, activation='sigmoid')
])
```
```python
model4.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
Output:
```python
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
  return dispatch_target(*args, **kwargs)
63/63 [==============================] - 15s 135ms/step - loss: 0.3351 - accuracy: 0.8975 - val_loss: 0.1088 - val_accuracy: 0.9604
Epoch 2/20
63/63 [==============================] - 7s 101ms/step - loss: 0.2913 - accuracy: 0.9115 - val_loss: 0.0878 - val_accuracy: 0.9678
Epoch 3/20
63/63 [==============================] - 6s 88ms/step - loss: 0.2903 - accuracy: 0.9140 - val_loss: 0.1135 - val_accuracy: 0.9579
Epoch 4/20
63/63 [==============================] - 6s 87ms/step - loss: 0.2432 - accuracy: 0.9230 - val_loss: 0.0902 - val_accuracy: 0.9691
Epoch 5/20
63/63 [==============================] - 6s 89ms/step - loss: 0.2271 - accuracy: 0.9275 - val_loss: 0.0857 - val_accuracy: 0.9715
Epoch 6/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3397 - accuracy: 0.8960 - val_loss: 0.1487 - val_accuracy: 0.9554
Epoch 7/20
63/63 [==============================] - 6s 88ms/step - loss: 0.3139 - accuracy: 0.9065 - val_loss: 0.0881 - val_accuracy: 0.9703
Epoch 8/20
63/63 [==============================] - 6s 87ms/step - loss: 0.2878 - accuracy: 0.9190 - val_loss: 0.0867 - val_accuracy: 0.9678
Epoch 9/20
63/63 [==============================] - 6s 88ms/step - loss: 0.2410 - accuracy: 0.9195 - val_loss: 0.0996 - val_accuracy: 0.9616
Epoch 10/20
63/63 [==============================] - 6s 88ms/step - loss: 0.2986 - accuracy: 0.9145 - val_loss: 0.0907 - val_accuracy: 0.9703
Epoch 11/20
63/63 [==============================] - 6s 86ms/step - loss: 0.2419 - accuracy: 0.9305 - val_loss: 0.0846 - val_accuracy: 0.9728
Epoch 12/20
63/63 [==============================] - 6s 86ms/step - loss: 0.2578 - accuracy: 0.9240 - val_loss: 0.0889 - val_accuracy: 0.9641
Epoch 13/20
63/63 [==============================] - 6s 86ms/step - loss: 0.3018 - accuracy: 0.9060 - val_loss: 0.0774 - val_accuracy: 0.9728
Epoch 14/20
63/63 [==============================] - 6s 87ms/step - loss: 0.2483 - accuracy: 0.9255 - val_loss: 0.1145 - val_accuracy: 0.9616
Epoch 15/20
63/63 [==============================] - 6s 88ms/step - loss: 0.2628 - accuracy: 0.9165 - val_loss: 0.0946 - val_accuracy: 0.9703
Epoch 16/20
63/63 [==============================] - 6s 87ms/step - loss: 0.2755 - accuracy: 0.9120 - val_loss: 0.0755 - val_accuracy: 0.9691
Epoch 17/20
63/63 [==============================] - 6s 87ms/step - loss: 0.3114 - accuracy: 0.9100 - val_loss: 0.1023 - val_accuracy: 0.9653
Epoch 18/20
63/63 [==============================] - 6s 87ms/step - loss: 0.2591 - accuracy: 0.9125 - val_loss: 0.1021 - val_accuracy: 0.9641
Epoch 19/20
63/63 [==============================] - 6s 86ms/step - loss: 0.2925 - accuracy: 0.9040 - val_loss: 0.1439 - val_accuracy: 0.9542
Epoch 20/20
63/63 [==============================] - 6s 88ms/step - loss: 0.3241 - accuracy: 0.9055 - val_loss: 0.1075 - val_accuracy: 0.9629
```
```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.show()
```
![HW3_model4.png]({{ site.baseurl }}/images/HW3_model4.png)

## §6. Score on Test Data
```python
model4.evaluate(test_dataset)
```
Output:
```python
6/6 [==============================] - 1s 83ms/step - loss: 0.0957 - accuracy: 0.9740
[0.09571874141693115, 0.9739583134651184]
```
In our last model, we got 97.4% which is pretty good!
You did a great job. Now it's time to go sleep :)

